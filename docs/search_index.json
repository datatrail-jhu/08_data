[["index.html", "08: Getting Data About this Course", " 08: Getting Data June, 2022 About this Course This course is part of a series of courses for DataTrail. DataTrail is a no-cost, paid 14-week educational initiative for young-adult, high school and GED-graduates. DataTrail aims to equip members of underserved communities with the necessary skills and support required to work in the booming field of data science. DataTrail is a fresh take on workforce development that focuses on training both Black, Indigenous, and other people of color (BIPOC) interested in the data science industry and their potential employers. Offered by the Johns Hopkins Bloomberg School of Public Health, in partnership with local non-profits and Leanpub, DataTrail combines a mutually-intensive learning experience (MILE) with a whole-person ecosystem of support to allow aspiring data scientists and their employers to succeed. DataTrail uses mutually-intensive learning DataTrail joins aspiring data science scholars and expert-level data scientist mentors in a mutually-intensive learning experience (MILE). In the DataTrail MILE: Scholars engage in cutting-edge technical and soft skills training needed to enter the data science field. Mentors engage in anti-racism and mentorship training needed to be impactful mentors and informed colleagues on diverse data science teams. The social connections created along the way will fuel job opportunities for scholars and foster a more diverse, equitable, and inclusive climate at the mentors’ institutions. { course-completeness: 100 course-attempts: 2 default-quiz-attempts: 2 default-random-choice-order: true default-quiz-show-answers: none } "],["where-does-data-come-from.html", "Chapter 1 Where Does Data Come From?", " Chapter 1 Where Does Data Come From? We’ve previously discussed that data are everywhere. Every time you buy something with a credit card or like a photo on Instagram, data is generated. Spreadsheets where you track your finances are data. The photos you upload to social media are data. The tweets people write are data. The information on your favorite website is data. Clearly, in today’s world, data are everywhere. And, as a data scientist, your job is to work with the data at your disposal to answer questions. Therefore, you need to know how to work with the data generated. In previous courses in this Course Set, you’ve learned how to work with data in RStudio Cloud; however, in most of these cases we’ve either used data that are available in R by default or have given you the code to directly read the data into RStudio Cloud. In this course, we’ll now formalize the various types of data that data scientists work with most often and teach you how to read each type of data into R. We’ll also spend a little bit of time talking about some less conventional types of data in slightly less detail so you’re aware packages that exist for working with these types of data, even if you aren’t working with them every day. We hope that having all this information in one place will help you not only understand the various file types but also provide you a resource to reference in the future as you work with various types of data! In this lesson we’ll briefly introduce a number of different types of data. In the subsequent lessons of this course we’ll walk through step-by-step on how to get these data into R, discuss where you can find data sets to work with, and discuss important information about the required safety, privacy, and ethical concerns that arise when working with data. 1.0.1 Basic Data files Data Scientists are often working with data stored in spreadsheets. This probably doesn’t come as a surprise, as most of the example data you’ve worked on throughout this Course Set so far has used data in this format. Often, these data are stored in comma-separated value (CSV) files. However, data of this format can also be stored in Google Sheets, Excel (.xlsx, .xls), and tab-delimited files (.txt), among others. We’ll define the various file formats and discuss how to get these types of data into R in the next few lessons in this course. 1.0.2 Data from the Internet In addition to data in the form of spreadsheets that you have stored on your computer or in Google Sheets, there is a lot of data on the Internet. Sometimes this is in the form of data tables that are on a website and that you want to be able to read into R. Sometimes, you want to pull information from a webpage and get it into a usable format for your own analysis. In these cases, there are often ways to access this information. 1.0.2.1 APIs Application Programming Interfaces (APIs) are, in the most general sense, software that allow to different web-based applications to communicate with one another over the Internet. APIs are used by software developers and programmers frequently to allow apps to communicate with one another; however, for our purposes APIs can be incredibly helpful for retrieving data. For example, if there was a dataset on the Internet that you wanted to use in RStudio Cloud, you could use an API to communicate with the website where those data are located, get access to the data, and then have the data in RStudio Cloud to work with. All of this is possible because of APIs, allowing for all of that communication between applications to occur without us even being aware. We’ll walk through the steps of getting data from the Internet via APIs in a later lesson in this course. 1.0.2.2 Web Scraping Sometimes, data are on a web page but not necessarily in a nice data table. For example, imagine that there is a list of individuals working at a company with a picture of each individual, their job title, and a description of what each person does at the company. Now, imagine that there are a lot of these websites with similar information. Now, imagine that you’re starting up a company and want to figure out what types of people you’ll need at this company by using information from what others have done at similar companies. You may want to obtain the name and job title of the individuals at each company from their websites so that you can compare what job titles are working at each company. To do this, you would obtain the necessary information from each website and get it into a usable format. This process is called web scraping. We’ll discuss how to do this in a future lesson in this course. 1.0.3 Relational Databases In addition to singular spreadsheets of data, data that are more complex than what can be stored in a single spreadsheet are often stored in what are known as relational databases. Relational databases contain multiple tables that are related to one another in some way. For example, consider a spreadsheet of all the restaurants in your area. This spreadsheet may contain information about the restaurant’s name, its location, and what type of food it serves. Now, imagine a separate spreadsheet containing information about food inspections. Each row could contain a different inspection, the date the inspection was performed, who performed the inspection, and information about how the restaurant performed. Each restaurant would have multiple inspections, assuming they restaurant has been open a while. This means that if you wanted to know where a restaurant was located and information about how that restaurant has performed on inspection, you’d have to go the first spreadsheet, find the information about the restaurant’s location there and then go search in the second spreadsheet and get all the rows that correspond to that restaurant’s inspections. This can all be accomplished with separate spreadsheets. However, these data can be more easily stored and searched using a relational database. In such a case, each restaurant would have a unique ID. This ID would be stored in both spreadsheets, allowing one to easily pull out all the desired information using that single ID. This is what we mean by relational. The information within the database is all related to one another (in our example, each contains information about restaurants); however, each spreadsheet within the database contains different information. You can pull out each type of information using the unique ID for the restaurant your interested in! Relational databases help keep track of data as datasets grow and new pieces of information are incorporated. We’ll discuss how to work with relational databases within R in a later lesson in this course. 1.0.4 Unconventional Data In addition to information stored in spreadsheets and relational databases or data scraped from the web, there are many other types of data that one could work with, but that data scientists typically work with less frequently. 1.0.4.1 Text Politicians give speeches. Authors write books. Everyone writes e-mails. All of these generate a lot of text. While not as straightforward as analyzing numbers, text too can be analyzed. From text, we can draw insights about what words have been used historically and how this has changed over time. We can use the words from a book to infer in what year that book was written, given the words used in the book. Or, we could analyze what words in an email elicit a response, and what words elicit that response most quickly. Thus, it’s important to understand that it’s possible to analyze text within RStudio Cloud. 1.0.4.2 Images Similarly, it won’t come as any surprise that images are all around us. Pictures are constantly being posted to social media, and nearly every online news story is accompanied by at least one image. The ability to analyze images allows Facebook to guess who you are when a picture is posted of you. And, it allows software programs to take to pictures and guess what those individuals’ kids may look like. The ability to analyze images is, as stated for text, not always the most straightforward, but is also possible within RStudio Cloud. 1.0.4.3 Audio Files We are constantly barraged with sound. We listen to music and podcasts through our phones, radios, and computers. And, we leave voicemails and voice messages for one another regularly. These generate audio files. Consider having access to audio files for thousands of popular songs. One could analyze these to group these files by genre and build a model that predicts which types of songs in the future will be the most popular. 1.0.5 Slides and Video Where Does Data Come From Slides "],["csv-excel-and-tsv-files.html", "Chapter 2 CSV, Excel, and TSV Files", " Chapter 2 CSV, Excel, and TSV Files In this lesson, we’ll discuss a few of the main file types used to store tabular data. To review briefly, tabular data are the type of data stored in spreadsheets. Information from different variables are stored in columns and each observation is stored in a different row. The values for each observation is stored in its respective cell. 2.0.1 CSV files Comma-separated values (CSV) files allow us to store tabular data in a simple format. CSVs are plain-text files, which means that all the important information in the file is represented by text (where text is numbers, letters, and symbols you can type on your keyboard). For example, consider a dataset that includes information about the heights and blood types of three individuals. You could make a table that has three columns (names, heights, and blood types) and three rows (one for each person) in Google Docs or Microsoft Word. However, there is a better way of storing this data in plain text without needing to put them in table format. CSVs are a perfect way to store these data. In the CSV format, the values of each column for each person in the data are separated by commas and each row (each person in our case) is separated by a new line. This means your data would be stored in the following format: sample CSV Notice that CSV files have a .csv extension at the end. You can see this above at the top of the file. One of the advantages of CSV files is their simplicity. Because of this, they are one of the most common file formats used to store tabular data. Additionally, because they are plain text, they are compatible with many different types of software. CSVs can be read by most programs. Specifically, for our purposes, these files can be easily read into R (or Google Sheets, or Excel), where they can be better understood by the human eye. Here you see the same CSV opened in Google Slides, where it’s more easily interpretable by the human eye: CSV opened in Google Slides As with any file type, CSVs do have their limitations. Specifically, CSV files are best used for data that have a consistent number of variables across observations. For example, in our example, there are three variables for each observation: “name”, “height”, and “blood_type”. If, however, you had eye color and weight for the second observation, but not for the other rows, you’d have a different number of variables for the second observation than the other two. This type of data is not best suited for CSVs. However, whenever you have information the same number of variables across all observations, CSVs are a good bet! 2.0.2 Downloading CSV files If you entered the same values used above into Google Slides first and wanted to download this file as a CSV to read into R, you would enter the values in Google slides, and then click on “File” and then “Download” as and choose “Comma-separated values (.csv, current sheet)”. The dataset that you created will be downloaded as a CSV file on your computer. Make sure you know the location of your file (if on a Chromebook, this will be in your “Downloads” folder). Download as CSV file 2.0.3 Reading CSV files into RStudio Cloud Now that you have a CSV file, let’s discuss how to get it into RStudio Cloud! Log in to your RStudio Cloud account. Create a new project. On the RStudio workspace that you see, click on Upload under Files on the bottom right corner of the screen. On the window that pops up click on Choose File. Upload a file on RStudio Cloud Now, find where you saved the file (for instance “Downloads”) and click on OPEN. After this, the window closes automatically and you’ll be back in your workspace on RStudio Cloud. You will see that your CSV file now appears among other files. (A reminder: if you were working on a data science project, this would go in your “raw_data” directory. For this example, however, we’ll keep it in “cloud/project”) Find local file Now, while the file has now been uploaded to your RStudio Cloud project, it’s important to recognize the file is not yet imported to your R environment as an object. We’ll do that now! The best way to accomplish this is using the function read_csv() from the readr package. (Note, if you haven’t installed the readr package, you’ll have to do that first.) Inside the parenthesis of the function, write the name of the file in quotes, including the file extension (.csv). Make sure you type the exact file name. Save the imported data in a data frame called df_csv. Your data will now be imported into R environment. If you use the command head(df_csv) you will see the first several rows of your imported data frame: ## install.packages(&quot;readr&quot;) library(readr) ## read CSV into R df_csv &lt;- read_csv(&quot;sample_data - Sheet1.csv&quot;) ## look at the object head(df_csv) read_csv() Above, you see the simplest way to import a CSV file. However, as with many functions, there are other arguments that you can set to specify how to import your specific CSV file, a few of which are listed below. However, as usual, to see all the arguments for this function, use ?read_csv within R. col_names = FALSE to specify that the first row does NOT contain column names. skip = 2 will skip the first 2 rows. You can set the number to any number you want. Helpful if there is additional information in the first few rows of your data frame that are not actually part of the table. n_max = 100 will only read in the first 100 rows. You can set the number to any number you want. Helpful if you’re not sure how big a file is and just want to see part of it By default, read_csv() converts blank cells to missing data (NA). Finally, we introduce the function read_csv here and recommend that you use it, as it is the simplest and fastest way to read CSV files into R. However, we note that there is a function read.csv which is available by default in R. You will likely see this function in others’ code, so we just want to make sure you’re aware of it. 2.0.4 Excel files While CSV files hold plain text as a series of values separated by commas, an Excel (or .xls or .xlsx) file holds information in a workbook that comprises both values and formatting (colors, conditional formatting, font size, etc.). You can think of Excel files a fancier CSV file. While this may sound appealing, we’ll remind you that CSV files can be read by many different pieces of software, Excel files can only be viewed in specific pieces of software, and thus are generally less flexible. That said, many people save their data in Excel, so it’s important to know how to work with them in RStudio Cloud. Let’s go back to the Google Sheet that we created and instead of downloading the file locally as as CSV, download it as Microsoft Excel (.xlsx) file. Download as Excel file Save the file where you can find it. Similar to the CSV file, first, upload the file into your RStudio Cloud workspace. 2.0.5 Reading Excel files into RStudio Cloud To read this file into R, we’ll have to use a different function than above, as this file is not a CSV file. We’ll use the read_excel() function from the readxl package. Install the package first and then use the function read_excel() from the package read the Excel file into your R Environment. As above, by default, read_excel() converts blank cells to missing data (NA). ## install and load package install.packages(&quot;readxl&quot;) library(readxl) df_excel &lt;- read_excel(&quot;sample_data.xlsx&quot;) head(df_excel) Find local file 2.0.6 Text files Another common form of data is text files that usually come in the form of TXT or TSV file formats. Like CSVs, text files are simple, plain-text files; however, rather than columns being separated by commas, they are separated by tabs (represented by “ in plain-text). Like CSVs, they don’t allow text formatting (i.e. text colors in cells) and are able to be opened on many different software platforms. This makes them good candidates for storing data. 2.0.7 Reading TSV files into RStudio Cloud The process for reading these files into R is similar to what you’ve seen so far. We’ll again use the readr package, but we’ll instead use the read_tsv() function. ## read TSV into R df_tsv &lt;- read_tsv(&quot;sample_data - Sheet1.tsv&quot;) ## look at the object head(df_tsv) 2.0.8 Reading TXT files into RStudio Cloud Sometimes, tab-separated files are saved with the .txt file extension. TXT files can store tabular data, but they can also store simple text. Thus, while TSV is the more appropriate extension for tabular data that are tab-separated, you’ll often run into tabular data that individuals have saved as a TXT file. In these cases, you’ll want to use the more generic read_delim() function from readr. Google Sheets does not allow tab-separated files to be downloaded with the .txt file extension (since .tsv is more appropriate); however, if you were to have a file “sample_data.txt” uploaded into your RStudio Cloud project, you could use the following code to read it into your R Environment, where “ specifies that the file is tab-delimited. ## read TXT into R df_txt &lt;- read_delim(&quot;sample_data.txt&quot;, delim = &quot;\\t&quot;) ## look at the object head(df_txt) This function allows you to specify how the file you’re reading is in delimited. This means, rather than R knowing by default whether or not the data are comma- or tab- separated, you’ll have to specify it within the argument delim in the function. read_delim() is a more generic version of read_csv(). What this means is that you could use read_delim() to read in a CSV file. You would just need to specify that the file was comma-delimited if you were to use that function. 2.0.9 Exporting Data in R to CSV The last topic of this lesson is about how to export data from R. So far we learned about reading data into R. However, sometimes you would like to share your data with others and need to export your data from R to some format that your collaborators can see. As discussed above, CSV format is a good candidate because of its simplicity and compatibility. Let’s say you have a data frame in the R environment that you would like to export as a CSV. To do so, you could use write_csv() from the readr package. Since both methods are fairly similar, let’s look at the one from readr package. Since we’ve already created a data frame named df_csv, we can export it to a CSV file using the following code. After typing this command, a new CSV file called my_csv_file.csv will appear in the Files section. write_csv(df_csv, path = &quot;my_csv_file.csv&quot;) You could similar save your data as a TSV file using the function write_tsv() function. We’ll finally note that there are default R functions write.csv() and write.table() that accomplish similar goals. You may see these in others’ code; however, we recommend sticking to the intuitive and quick readr functions discussed in this lesson. 2.0.10 Slides and Video CSV, Excel, and TSV Files Slides "],["importing-data-from-google-sheets.html", "Chapter 3 Importing Data from Google Sheets", " Chapter 3 Importing Data from Google Sheets In the last lesson we discussed how to read various file types into R. In each of these examples, we first had to upload the file into our RStudio Cloud project before we could read the file into R using one of the read_*() functions. What if we could cut out that first step, eliminating the need to upload the file into our project? This way, we could just use one function to read data directly into RStudio Cloud. This is capable thanks to data stored Google Sheets and the fantastic R package googlesheets. Additionally, using this approach, we can read in and analyze data hosted on Google Sheets in real-time. This means that as the data are updated over time, you can read the data into R and analyze it in its current state. Then, when the data are updated, you just re-run your code, and you’ll get an updated analysis. To see what we mean specifically, let’s take a look at an example. Imagine you’ve sent out a survey to your friends asking about how they spend their day. Let’s say you’re mostly interested in knowing the hours spent on work, leisure, sleep, eating, socializing, and other activities. So in your Google Sheet you add these six items as columns and one column asking for your friends names. To collect this data, you then share the link with your friends, giving them the ability to edit the Google Sheet. Survey Google Sheets Your friends will then one-by-one complete the survey. And, because it’s a Google Sheet, everyone will be able to update the Google Sheet, regardless of whether or not someone else is also looking at the Sheet at the same time. As they do, you’ll be able to pull the data and import it to R for analysis at any point. You won’t have to wait for everyone to respond. You’ll be able to analyze the results in real-time by directly reading it into R from Google Sheets, avoiding the need to download it each time you do so. In other words, every time you import the data from the Google Sheets link using the googlesheets package, the most updated data will be imported. Let’s say, after waiting for a week, your friends’ data look something like this: Survey Data 3.0.1 The googlesheets package The googlesheets package allows R users to take advantage of the Google Sheets API. In the first lesson in this course we mentioned that an API allows different application to communicate with one another. In this case, Google has released an API that allows other software to communicate with Google Sheets and retrieve data and information directly from Google Sheets. The googlesheets package enables R users (you!) to then easily access that API and retrieve your Google Sheets data. Using this package is is the best and easiest way to analyze and edit Google Sheets data in R. In addition to the ability of pulling data, you can also edit a Google Sheet or create new sheets. Like any other package, we first need to install and attach the package. 3.0.1.1 Getting Started with googlesheets install.packages(&quot;googlesheets&quot;) library(googlesheets) Now, let’s get to importing your survey data into R. Every time you start a new session, you need to authenticate the use of googlesheets package with your Google account. This is a great features as it ensures that you want to allow access to your Google Sheets and allows the Google Sheets API to make sure that you should have access to the files you’re going to try to access. The command gs_auth(new_user = TRUE) will open a new page in your browser that asks you which Google account’s Google Sheets you’d like to give access to. Click on the appropriate Google user to provide googlesheets access to the Google Sheets API. Authenticate After you click “ALLOW”, giving permission for the googlesheets package to connect to your Google account, you will likely be shown a screen where you will be asked to copy an authentication code. Copy this authentication code and paste it in your RStudio Cloud console. Allow 3.0.1.2 Navigating googlesheets: gs_ls() and gs_title() Once authenticated, you can use the command gs_ls() to list all your worksheets on Google Sheets as a table. In order to access a specific sheet, in this case the sheet that you created and named “survey”, you need to use the function gs_title(). We’ll assign this information to the object survey_sheet. survey_sheet &lt;- gs_title(&quot;survey&quot;) If you type the name of the sheet correctly, you will see a message that says Sheet successfully identified: “survey”. Sheet successfully identified 3.0.1.3 Reading data in using googlesheets: gs_read() At this point, you can read the data into R using the function gs_read() with the survey_sheet object output from gs_title() as your input to gs_read(): survey_data &lt;- gs_read(survey_sheet) Sheet successfully read into R There are additional (optional) arguments to gs_read(), some are similar to those in read_csv() and read_excel(), while others are more specific to reading in Google Sheets: skip = 1 : will skip the first row of the Google Sheet ws = 1 : specifies that you want googlesheets to read in the first worksheet in your Google Sheet col_names = FALSE : specifies that the first row is not column names range = \"A1:G5\" : specifies the range of cells that we like to import is A1 to G5. n_max = 100 : specifies the maximum number of rows that we want to import is 100. 3.0.1.4 Adding rows and editing cells In addition to reading in data from Google Sheets directly using the googlesheets package, you can also modify your Google Sheet directly through R. For example, you can edit a cell or add a row to your sheet. Let’s say you’d like to add your own respond to the survey. For this you can use the command gs_add_row(). Note that the input argument specifies what you would like to add in the new row. If everything goes well, you will get a message saying Row successfully appended. my_response &lt;- c(&quot;Me&quot;, 10, 8, 2, 2, 1, 1) gs_add_row(survey_sheet, input = my_response) gs_add_row() If you now check the sheet on Google Sheets, you will see the appended row. Survey Data with Added Row To edit a specific cell you can use the command gs_edit_cell() but you will have to tell googlesheets which specific cell to edit. Let’s say we want to change the value of the cell D4 from 0 to 2. For this we will use the gs_edit_cells() function. The anchor argument points to the cell that we want to modify and the input argument contains the new value that we want to assign to the cell. gs_edit_cells(survey_sheet, anchor = &quot;D4&quot;, input = 2) gs_edit_cells() If you were to return to your Google Sheets, you’d notice that cell D4 now has a 2, rather than a 0 in it! 3.0.2 Importing CSV files from the web This lesson has focused on getting data from Google Sheets into RStudio Cloud; however, sometimes the data you need to work with is elsewhere online but you’d like to store it in a Google Sheet. While we will later learn that you can import them directly into R, it’s good to know that you can also import CSV files hosted on the web directly into Google Sheets. For instance, the dataset here is a CSV file containing countries GDPs (national incomes). In order to import the file directly to Google Sheets, open a blank Google Sheets document as we learned before. In the first cell (top left corner) type in =importData(\"https://raw.githubusercontent.com/datasets/gdp/master/data/gdp.csv\"). Make sure to include the equal sign (=) in the beginning. importing data directly into Google Sheets This will import all the data to your Google Sheets document. You can then use the steps discussed above any time you want to work with these data in RStudio Cloud. data in Google Sheets 3.0.3 Summary In this lesson, we’ve introduced the R package googlesheets. We’ve discussed the need for authentication and have demonstrated a number of its basic capabilities. We’ve walked through an example of how to read a Google Sheet into R and how to add and edit content directly to your Google Sheet from R. We also briefly covered how to import CSV data into Google Sheets directly. While we’ve covered the basics, there are certainly a number of additional capabilities of the package, so feel free to explore even more! 3.0.4 Additional Resources googlesheets on GitHub - includes an incredibly helpful README from Jenny Bryan on the basic functions of the package as well as a table with brief descriptions of all the functions googlesheets vignette - a more in-depth tutorial at the packages’ functionality from Jenny Bryan and Joanna Zhao 3.0.5 Slides and Video Importing Data from Google Sheets Slides "],["getting-data-from-the-internet.html", "Chapter 4 Getting Data From the Internet", " Chapter 4 Getting Data From the Internet In the introductory lesson, we mentioned that there is a lot of data on the Internet, which probably comes at no surprise given the vast amount of information on the Internet. Sometimes these data are in a nice CSV format that we can quickly pull from the Internet. Sometimes, the data are spread across a web page, and it’s our job to “scrape” that information from the webpage and get it into a usable format. Knowing first that this is possible within R and second, having some idea of where to start is an important start to beginning to get data from the Internet. We’ll walk through three R packages in this lesson to help get you started in getting data from the Internet. So, before we jump in, we’ll have you install these packages: ## Install packages install.packages(c(&quot;httr&quot;, &quot;rvest&quot;, &quot;jsonlite&quot;)) 4.0.1 API access In the first lesson we mentioned that Application Programming Interfaces (APIs) are, in the most general sense, software that allow to different web-based applications to communicate with one another over the Internet. Modern APIs conform to a number of standards. This means that many different applications are using the same approach, so a single package in R is able to take advantage of this and communicate with many different applications, as long as the application’s API adheres to this generally agreed upon set of “rules”. The R package that we’ll be using to acquire data and take advantage of this is called httr. This package name suggests that this is an “R” package for “HTTP”. So, we know what R is, but what about HTTP? You’ve probably seeing HTTP before at the start of web addresses, (ie http://www.gmail.com), so you may have some intuition that HTTP has something to do with the Internet, which is absolutely correct! HTTP stands for Hypertest Transfer Protocol. In the broadest sense, HTTP transactions allow for messages to be sent between two points on the Internet. You, on your computer can request something from a web page, and the protocol (HTTP) allows you to connect with that webpage’s server, do something, and then return you whatever it is you asked for. Working with a web API is similar to accessing a website in many ways. When you type a URL (ie www.google.com) into your browser, information is sent from your computer to your browser. Your browser then interprets what you’re asking for and displays the website you’ve requested. Web APIs work similarly. You request some information from the API and the API sends back a response. The httr package will hep you carry out these types of requests within R. Let’s stop talking about it, and see an actual example! HTTP access via httr 4.0.2 Getting Data: httr HTTP is based on a number of important verbs : GET(), HEAD(), PATCH(), PUT(), DELETE() and POST(). For the purposes of retrieving data from the Internet, you may be able to guess which verb will be the most important for our purposes! GET() will allow us to fetch a resource that already exists. We’ll specify a URL to tell GET() where to go look for what we want. While we’ll only highlight GET() in this lesson, for full understanding of the many other HTTP verbs and capabilities of httr, refer to the additional resources provided at the end of this lesson. GET() will access the API, provide the API with the necessary information to request the data we want, and retrieve some output. API requests are made to an API endpoint to get an API response 4.0.3 Example 1: GitHub’s API The example is based on a wonderful blogpost from Tyler Clavelle. In this example, we’ll use will take advantage of GitHub’s API, because it’s accessible to anyone. Other APIs, while often freely-accessible, require credentials, called an API key. We’ll talk about those later, but let’s just get started using GitHub’s API now! 4.0.3.1 API Endpoint The URL you’re requesting information from is known as the API endpoint. The documentation from GitHub’s API explains what information can be obtained from their API endpoint: https://api.github.com. That’s the base endpoint, but if you wanted to access a particular individual’s GitHub repositories, you would want to modify this base endpoint to: https://api.github.com/users/username/repos, where you would replace username with your GitHub username. 4.0.3.2 API request: GET() Now that we know what our API endpoint is, we’re ready to make our API request using GET(). The goal of this request is to obtain information about what repositories are available in your GitHub account. To use the example below, you’ll want to change the username janeeverydaydoe to your GitHub username. ## load package library(httr) library(dplyr) ## Save GitHub username as variable username &lt;- &#39;janeeverydaydoe&#39; ## Save base endpoint as variable url_git &lt;- &#39;https://api.github.com/&#39; ## Construct API request api_response &lt;- GET(url = paste0(url_git, &#39;users/&#39;, username, &#39;/repos&#39;)) Note: In the code above, you see the function paste0(). This function concatenates (links together) each the pieces within the parentheses, where each piece is separated by a comma. This provides GET() with the URL we want to use as our endpoints! httr code to access GitHub 4.0.3.3 API response: content() Let’s first take a look at what other variables are available within the api_response object: ## See variables in response names(api_response) httr response While we see ten different variables within api_response, we should probably first make sure that the request to GitHub’s API was successful. We can do this by checking the status code of the request, where “200” means that everything worked properly: ## Check Status Code of request api_response$status_code But, to be honest, we aren’t really interested in just knowing the request worked. We actually want to see what information is contained on our GitHub account. To do so we’ll take advantage of httr’s content() function, which as its name suggests, extracts the contents from an API request. ## Extract content from API response repo_content &lt;- content(api_response) httr status code and content() You can see here that the length of repo_content in our case is 6 by looking at the Environment tab. This is because the GitHub account janeeverydaydoe had six repositories at the time of this API call. We can get some information about each repo by running the function below: ## function to get name and URL for each repo lapply(repo_content, function(x) { df &lt;- data_frame(repo = x$name, address = x$html_url)}) %&gt;% bind_rows() output from API request Here, we’ve pulled out the name and URL of each repository in Jane Doe’s account; however, there is a lot more information in the repo_content object. To see how to extract more information, check out the rest of Tyler’s wonderful post here. 4.0.4 Example 2: Obtaining a CSV In a previous course in this Course Set on Data Visualization, we assigned a project where the data for the project had already been saved on RStudio Cloud for you in CSV format. However, these data are available on the Internet. Now that we know how to use httr and have an understanding about APIs, let’s do that now! So, the data are available for download from this link: data.fivethirtyeight.com, but are also hosted on GitHub here, and we will want to use the specific URL for this file: https://raw.githubusercontent.com/fivethirtyeight/data/master/steak-survey/steak-risk-survey.csv in our GET() request. steak-survey on GitHub To do so, we would do the following: ## Make API request api_response &lt;- GET(url = &quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/steak-survey/steak-risk-survey.csv&quot;) ## Extract content from API response df_steak &lt;- content(api_response, type=&quot;text/csv&quot;) GET() steak-survey CSV Here, we again specify our url within GET() followed by use of the helpful content() function from httr to obtain the CSV from the api_response object. df_steak includes the data from the CSV directly from the GitHub API, without having to download the data first! 4.0.5 API keys Not all API’s are as “open” as GitHub’s. For example, if you ran the code for the first example above exactly as it was written (and didn’t change the GitHub username), you would have gotten information about the repos in janeeverydaydoe’s GitHub account. Because it is a fully-open API, you’re able to retrieve information about not only your GitHub account, but also other users’ public GitHub activity. This makes good sense because sharing code among public repositories is an important part of GitHub. Alternatively, while Google also has an API (or rather, many API’s), they aren’t quite as open. This makes good sense. There is no reason I should have access to the files on someone else’s Google Drive account. Controlling whose files one can access through Google’s API is an important privacy feature. In these cases, what is known as a key is required to gain access to the API. API keys are obtained from the website’s API site (ie, for Google’s APIs, you would start here. Once acquired, these keys should never be shared on the Internet. There is a reason they’re required, after all. So, be sure to never push a key to GitHub or share it publicly. (If you do ever accidentally share a key on the Internet, return to the API and disable the key immediately.) For example, to access the Twitter API, you would obtain your key and necessary tokens from Twitter’s API and replace the text in the key, secret, token and token_secret arguments below. This would authenticate you to use Twitter’s API to acquire information about your home timeline. myapp = oauth_app(&quot;twitter&quot;, key = &quot;yourConsumerKeyHere&quot;, secret = &quot;yourConsumerSecretHere&quot;) sig = sign_oauth1.0(myapp, token = &quot;yourTokenHere&quot;, token_secret = &quot;yourTokenSecretHere&quot;) homeTL = GET(&quot;https://api.twitter.com/1.1/statuses/home_timeline.json&quot;, sig) 4.0.6 Web Scraping Now that we’ve walked through two cases of obtaining data using an API and discussed API keys, let’s transition a little bit to talking about how to pull pieces of data from a website, when the data aren’t (yet!) in the format that we want them. In the first lesson of this course, we talked about the example of wanting to start a company but not knowing exactly what people you’ll need. So, you go to the websites of a bunch of companies similar to the company you start and pull off all the names and titles of the people working there. You then compare the titles across companies and voila, you’ve got a better idea of who you’ll need at your new company. You could imagine that while this information may be helpful to have, getting it manually would be a pain. Navigating to each site individually, finding the information, copying and pasting each name. That sounds awful! Thankfully, there’s a way to scrape the web from R directly! This uses the helpful package rvest. It gets its name from the word “harvest.” The idea here is you’ll use this package to “harvest” information from websites! However, as you may imagine, this is less straightforward than pulling data that are already formatted the way you want them (as we did above), since we’ll have to do some extra work to get everything in order. 4.0.6.1 rvest basics When rvest is given a webpage (URL) as input, an rvest function reads in the HTML code from the webpage. HTML is the language websites use to display everything you see on the website. You’ve seen HTML documents before, as this is one of the formats that you can Knit to from an R Markdown (.Rmd) document! Generally, all HTML documents require each webpage to have a similar structure. This structure is specified by using different tags. For example, a header at the top of your webpage would use a specific tag. Website links would use a different tag. These different tags help to specify how the website should appear. rvest takes advantage of these tags to help you extract the parts of the webpage you’re most interested in. So let’s see exactly how to do that all of this with an example. Different tags are used to specify different parts of a website 4.0.6.2 SelectorGadget To use rvest, there is a tool that will make your life a lot easier. It’s called SelectorGadget. It’s a “javascript bookmarklet.” What this means for us is that we’ll be able to go to a webpage, turn on SelectorGadget, and help figure out how to appropriately specify what components from the webpage we want to extract using rvest. To get started using SelectorGadget, you’ll have to enable the Chrome Extension. To enable SelectorGadget: Click here to open up the SelectorGadget Chrome Extension Click “ADD TO CHROME” ADD TO CHROME Click “Add extension” Add extension SelectorGadget’s icon will now be visible to the right of the web address bar within Google Chrome. You will click on this to use SelectorGadget in the example below. SelectorGadget icon 4.0.6.3 Web Scraping Example Similar to the example above, what if you were interested in knowing a few recommended R packages for working with data. Sure, you could go to a whole bunch of websites and Google and copy and paste each one into a Google Sheet and have the information. But, that’s not very fun! Alternatively, you could write and run a few lines of code and get all the information that way! We’ll do that in the example below. 4.0.6.3.1 Using SelectorGadget To use SelectorGadget, navigate to the webpage we’re interested in scraping: http://jhudatascience.org/stable_website/webscrape.html and toggle SelectorGadget by clicking on the SelectorGadget icon. A menu at the bottom-right of your web page should appear. SelectorGadget icon on webpage of interest Now that SelectorGadget has been toggled, as you mouse over the page, colored boxes should appear. We’ll click on the the name of the first package to start to tell SelectorGadget which component of the webpage we’re interested in. SelectorGadget selects strong on webpage of interest An orange box will appear around the component of the webpage you’ve clicked. Other components of the webpage that SelectorGadget has deemed similar to what you’ve clicked will be highlighted. And, text will show up in the menu at the bottom of the page letting you know what you should use in rvest to specify the part of the webpage you’re most interested in extracting. Here, we see with that SelectorGadget has highlighted the package names and nothing else! Perfect. That’s just what we wanted. Now we know how to specify this element in rvest! 4.0.6.3.2 Using rvest Now we’re ready to use rvest’s functions. We’ll use read_html() to read in the HTML from our webpage of interest. We’ll then use html_nodes() to specify which parts of the webpage we want to extract. Within this function we specify “strong”, as that’s what SelectorGadget told us to specify to “harvest” the information we’re interested in. Finally html_text extracts the text from the tag we’ve specified, giving us that list of courses we wanted to see! ## load package library(rvest) ## provide URL courses &lt;- read_html(&quot;http://jhudatascience.org/stable_website/webscrape.html&quot;) ## Get Courses courses %&gt;% html_nodes(&quot;strong&quot;) %&gt;% html_text() rvest code With just a few lines of code we have the information we were looking for! List of packages 4.0.6.4 A final note: SelectorGadget SelectorGadget selected what we were interested in on the first click in the example above. However, there will be times when it makes its guess and highlights more than what you want to extract. In those cases, after the initial click, click on any one of the items currently highlighted that you don’t want included in your selection. SelectorGadget will mark that part of the webpage in red and update the menu at the bottom with the appropriate text. To see an example of this, watch this short video here. 4.0.7 JSON In talking about APIs and API request calls, we left out a tiny bit of information. API requests generally return data in one of two data formats: JSON or XML. We’ll discuss a bit more about each these data formats in future lessons. However, as this is a lesson about obtaining information from the Internet, we’d be remiss not to mention that there are ways to work with JSON data in R, using the package jsonlite. Very briefly here (more to come later!), JSON (JavaScript Object Notation) is a text-based way to send information between a browser and a server. JSON is easy for humans to read and to write. Thus, it makes sense that API calls would return information in the JSON format. JSON data adhere to certain rules in how they are structured. For simplicity, JSON format requires objects to be comprised of key-value pairs. For example, in the case of: {\"Name\": \"Isabela\"}, “Name” would be a key, “Isabela” would be a value, and together they would be a key-value pair. Let’s take a look at how JSON data looks in R. library(jsonlite) ## generate a JSON object json &lt;- &#39;[ {&quot;Name&quot; : &quot;Woody&quot;, &quot;Age&quot; : 40, &quot;Occupation&quot; : &quot;Sherriff&quot;}, {&quot;Name&quot; : &quot;Buzz Lightyear&quot;, &quot;Age&quot; : 34, &quot;Occupation&quot; : &quot;Space Ranger&quot;}, {&quot;Name&quot; : &quot;Andy&quot;, &quot;Occupation&quot; : &quot;Toy Owner&quot;} ]&#39; ## take a look json This format cannot, as it is, be easily worked with within R; however, there is an R package to take data from JSON format to an object-based format, that can be worked with in R. The R package jsonlite is just what to work with whenever you have data in JSON format. When using the defaults of the function fromJSON(), jsonlite will take the data from JSON array format and helpfully return a data frame. ## take JSON object and covert to a data frame mydf &lt;- fromJSON(json) ## take a look mydf fromJSON() Data frames can also be returned to their original JSON format using the function: toJSON(). ## take JSON object and convert to a data frame json &lt;- toJSON(mydf) toJSON() Here, we’ve just briefly touched on what JSON format is; however, in a future lesson we’ll discuss this in greater detail. For now, however, it’s important to know that the jsonlite package is there for you whenever you have JSON data. And, it has two very helpful functions (among other functions in the package!): toJSON() and fromJSON() for such situations. 4.0.8 Keeping Track of your Data So far, we’ve discussed how to work with data you’ve obtained from the Internet. However, things on the Internet can change. If you’re not downloading the file to your system directly, you won’t have a static copy saved on your system. This means that while you’re saving space on your computer and not creating unnecessary copies of an identical file, the next time you go to access the file, it may not be there or it may have been updated. For these reasons, it’s incredibly important to record the date of when you acquired the data. This can be recorded in your R Markdown file where you’re doing your analysis or your data science lab notebook (to be discussed in a later lesson). Regardless, however, a record of the date you acquired the data is incredibly important. 4.0.9 Summary In this lesson we’ve covered a lot! We’ve gone through the basics of accessing API’s and scraping data from websites. And, we’ve touched on how to work with data that come to you in JSON format. Knowing what R is capable is a big part of this lesson. And, getting comfortable with the few examples here will be helpful when you branch out and start to use these packages on your own in the future! As we’ve just scraped the surface in this lesson, take a look at the additional resources below to get a better understanding of these topics! 4.0.10 Additional Resources 4.0.10.1 httr HTTP basics httr: Quick Start Guide httr tutorial, by Tyler Clavelle GitHub API Documentation 4.0.10.2 rvest rvest tutorial, from RStudio rvest documentation SelectorGadget documentation + video 4.0.10.3 jsonlite jsonlite: Getting Started blogpost on jsonlite, by Jeroen Ooms JSON information JSON : What It Is, How It Works, &amp; How to Use It 4.0.11 Slides and Video Getting Data From the Internet Slides "],["relational-data.html", "Chapter 5 Relational Data", " Chapter 5 Relational Data In lessons up to the point in the Course Set, the analyses you’ve done have either been carried out with a single dataset (i.e. data from a single Google Sheet) or the data have been compiled into a single data frame (i.e. data gathered from the Internet from an API request). However, there will be many cases as a data scientist where the data for your project will stored across a number of different spreadsheets that are all related to one another. In this lesson, we’ll discuss what relational data are, why you would want to store data in this way, and how to work with these types of data in RStudio Cloud. 5.0.1 Relational Data Relational data can be thought of as information being stored across many tables, with each table being related to all the other tables. Each table is linked to every other table by a set of unique identifiers. Relational data are related by unique identifiers To better understand this, let’s consider a toy example. We mentioned this example in the introductory lesson in this course, and we’ll return to it now. Consider a town where you have a number of different restaurants. In one table you have information about these restaurants including, where they are located and what type of food they serve. You then have a second table where information about health and safety inspections is stored. Each inspection is a different row and the date of the inspection, the inspector, and the safety rating are stored in this table. Finally, you have a third table. This third table contains information pulled from an API, regarding the number of stars given to each restaurant, as rated by people online. Each table contains different bits of information; however, there is a common column id in each of the tables. This allows the information to be linked between the tables. The restaurant with the id “JJ29JJ” in the restaurant table would refer to the same restaurant with the id “JJ29JJ” in the health inspections table, and so on. The values in this id column are known as unique identifiers because they uniquely identify each restaurant. No two restaurants will have the same id, and the same restaurant will always have the same id, no matter what table you’re looking at. The fact that these tables have unique identifiers connecting each table to all the other tables makes this example relational data. Unique identifiers help link entries across tables 5.0.1.1 Why relational data? Storing data in this way has a number of advantages; however, the three most important are: Efficient Data Storage Avoids Ambiguity Privacy Efficient Data Storage - By storing each bit of information in a separate table, you limit the need to repeat information. Taking our example above, imagine if we included everything in a single table. This means that for each inspection, we would copy and paste the restaurant’s address, type, and number of stars every time the facility is inspected. If a restaurant were inspected 15 times, this same information would be unnecessarily copy and pasted in each row! To avoid this, we simply separate out the information into different tables and relate them by their unique identifiers. Avoids Ambiguity - Take a look at the first table: “restaurant” here. You may notice there are two different restaurants named “Taco Stand.” However, looking more closely, they have a different id and a different address. They’re even different types of restaurants. So, despite having the same name, they actually are two different restaurants. The unique identifier makes this immediately clear! Unique identifiers in relational data avoid ambiguity Privacy - In using relational data, if there is ever information that is private and only some people should have access to, using this system simplifies that. You can restrict access to some of the data to ensure only those who should have access are able to access the data. 5.0.2 Relational Databases: SQL Now that we have an idea of what relational data are, let’s spend a second talking about how relational data are stored. Relational data are stored in databases. The most common database is SQLite. In order to work with data in databases, there has to be a way to query or search the database for the information you’re interested in. SQL queries search through SQLite databases and return the information you ask for in your query. For example, a query of the above example may look to obtain information about any restaurant that was inspected after July 1st of 2018. One would then use SQL commands to carry out this query and return the information requested. While we won’t be discussing how to write SQL commands in-depth here, we will be discussing how to use the R package RSQLite to connect to an SQLite database using RSQLite and how to work with relational data using dplyr and dbplyr 5.0.3 Connecting to Databases: RSQLite To better understand databases and how to work with relational data, let’s just start working with data from a database! The data we’ll be using are from a database frequently used to practice working with relational data: chinook.db. The database includes 11 tables with data that represents a digital media store. The data includes information generally related to related to media, artists, artists’ work, and those who purchase artists’ work (customers). More information about the details of how the tables are related to one another can be found here. For our purposes though, we’re only going to only describe two of the tables we’ll be using in our example in this lesson. We’re going to be looking at data in the artists and albums tables. artists and albums are related by the column ArtistId. relationship between two tables in the chinook database Without any more details, let’s get to it! Here you’ll see the code to install and load the RSQLite package. You’ll then download the chinook sample database, connect to the database, and first obtain a list the tables in the database: ## install and load packages ## this may take a minute or two install.packages(&quot;RSQLite&quot;) library(RSQLite) library(httr) ## specify driver sqlite &lt;- dbDriver(&quot;SQLite&quot;) ## download data url &lt;- &quot;http://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip&quot; GET(url, write_disk(tf &lt;- tempfile(fileext = &quot;.zip&quot;))) unzip(tf) ## Connect to Database db &lt;- dbConnect(sqlite, &#39;chinook.db&#39;) ## list tables in database dbListTables(db) The output from dbListTables() will include 13 tables. Among them will be the two tables we’re going to work through in our example: artists, and albums. output from dbListTables(db) In this example, we’re downloading a database and working with the data locally. However, more often, when working with SQLite databases, you’ll be connecting remotely. Using the RSQLite package is particularly helpful in this case because it allows you to connect to and query the database from R without reading all the data in. This is helpful in the case of very large databases, where you’ll want to avoid copying all the data and will instead want to only work with the parts of the database you need. 5.0.4 Working with Relational Data: dplyr &amp; dbplyr To access these tables within RStudio Cloud, we’ll have to install the packages dbplyr, which enables us to access the parts of the database we’re going to be working with. dbplyr allows you to use the same functions you learned when working with dplyr; however, it allows you to use these functions with a database. While dbplyr has to be loaded to work with databases, you likely won’t notice that you’re using it beyond that. Otherwise, you’ll just work with the files as if you were working with dplyr functions! After installing and loading dbplyr, we’ll be able to use the helpful tbl() function to extract the two tables we’re interested in working with! ## install and load packages install.packages(&quot;dbplyr&quot;) library(dbplyr) library(dplyr) ## get two tables albums &lt;- tbl(db, &quot;albums&quot;) artists &lt;- tbl(db, &quot;artists&quot;) 5.0.5 Mutating Joins Mutating joins allow you to take two different tables and combine the variables from both tables. This requires that each table have a column relating the tables to one another (i.e. a unique identifier). This unique identifier is used to match observations between the tables. However, when combining tables, there are a number of different ways in which the tables can be joined. We touched on this in the Data Tidying course; however, there we only covered left joins. Now, in this lesson, we’ll cover the following types of joins: Inner Join - only keep observations found in both x and y Left Join - keep all observations in x Right Join - keep all observations in y Full Join - keep any observations in x or y Let’s break down exactly what we mean by this using just a small toy example from the artists and albums tables from the chinook database. Here you see three rows from the artists table and four rows from the albums table small parts of the albums and artist tables 5.0.5.1 Inner Join When talking about inner joins, we are only going to keep an observation if it is found in all of the tables we’re combining. Here, we’re combining the tables based on the ArtistId column. In our dummy example, there are only two artists that are found in both tables. These are highlighted in green and will be the rows used to join the two tables. Then, once the inner join happens, only these artists’ data will be included after the inner join. inner join output will include any observation found in both tables In our toy example, when doing an inner_join(), data from any observation found in all the tables being joined are included in the output. Here, ArtistIds “1” and “2” are in both the artists and albums tables. Thus, those will be the only ArtistIds in the output from the inner join. And, since it’s a mutating join, our new table will have information from both tables! We now have ArtistId, Name, AlbumId, and Title in a single table! We’ve joined the two tables, based on the column ArtistId! inner join includes observations found in both tables Throughout this lesson we will use the coloring use see here to explain the joins, so we want to explain it explicitly here. Green cells are cells that will be used to make the merge happen and will be included in the resulting merged table. Blue cells are information that comes from the artists table that will be included after the merge. Red cells are pieces of information that come from the albums table that will be included after the merge. Finally, cells that are left white in the artists or albums table are cells that will not be included in the merge while cells that are white after the merge are NAs that have been added as a result of the merge. Now, to run this for our tables from the database, rather than just for a few rows in our toy example, you would do the following: ## do inner join inner &lt;- inner_join(artists, albums) ## look at output as a tibble as_tibble(inner) inner_join() output 5.0.5.2 Left Join For a left join, all rows in the first table specified will be included in the output. Any row in the second table that is not in the first table will not be included. In our toy example this means that ArtistIDs 1, 2, and 3 will be included in the output; however, ArtistID 4 will not. left join will include all observations found in the first table specified Thus, our output will again include all the columns from both tables combined into a single table; however, for ArtistId 3, there will be NAs for AlbumId and Title. NAs will be filled in for any observations in the first table specified that are missing in the second table. left join will fill in NAs Now, to run this for our tables from the database, rather than just for a few rows in our toy example, you would do the following: ## do left join left &lt;- left_join(artists, albums) ## look at output as a tibble as_tibble(left) left_join() output 5.0.5.3 Right Join Right Join is similar to what we just discussed; however, in the output from a right join, all rows in the final table specified are included in the output. NAs will be included for any observations found in the last specified table but not in the other tables. In our toy example, that means, information about ArtistIDs 1, 2, and 4 will be included. right join will include all observations found in the last table specified Again, in our toy example, we see that right_join() combines the information across tables; however, in this case, ArtistId 4 is included, but Name is an NA, as this information was not in the artists table for this artist. right join will fill in NAs Now, to run this for our tables from the database, you would have to do something slightly different than what you saw above. Note in the code below that we have to change the class of the tables from the database into tibbles before doing the join. This is because SQL does not currently support right or full joins, but dplyr does. Thus, we first have to be sure the data are a class that dplyr can work with using as_tibble(). Other than that, the code below is similar to what you’ve seen already: ## do right join right &lt;- right_join(as_tibble(artists), as_tibble(albums)) ## look at output as a tibble as_tibble(right) right_join() output While the output may look similar to the output from left_join(), you’ll note that there are a different number of rows due to how the join was done. The fact that 347 rows are present with the right join and 418 were present after the left join suggests that there are artists in the artists table without albums in the albums table. 5.0.5.4 Full Join Finally, a full join will take every observation from every table and include it in the output. full join will include any observation found in either table Thus, in our toy example, this join produces five rows, including all the observations from either table. NAs are filled in when data are missing for an observation. full join will fill in NAs As you saw in the last example, to carry out a full join, we have to again specify that the objects are tibbles before being able to carry out the join ## do right join full &lt;- full_join(as_tibble(artists), as_tibble(albums)) ## look at output as a tibble as_tibble(full) full_join() output 5.0.5.5 Mutating Joins Summary Now that we’ve walked through a number of examples of mutating joins, cases where you’re combining information across tables, we just want to take a second to summarize the four types of joins discussed using a visual frequently used to explain the most common mutating joins where each circle represents a different table and the gray shading on the venn diagrams indicates which observations will be included after the join. mutating joins summary To see a visual representation of this, there is a great resource on GitHub, where these joins are illustrated, so feel free to check out this link from Garrick Aden-Buie animating joins within relational data 5.0.6 Filtering Joins While we discussed mutating joins in detail, we’re just going to mention the ability to carry out filtering joins. While mutating joins combined variables across tables, filtering joins affect the observations, not the variables. This still requires a unique identifier to match the observations between tables. Filtering joins keep observations in one table based on the observations present in a second table. Specifically: semi_join(x, y) : keeps all observations in x with a match in y. anti_join(x, y) : keeps observations in x that do NOT have a match in y. In our toy example, if the join semi_join(artists, albums) were run, this would keep rows of artists where the ArtistID in artist was also in the albums table. semi_join() output Alternatively, anti_join(artists, albums) would output the rows of artists whose ArtistId was NOT found in the albums table. anti_join() output Note that in the case of filtering joins, the number of variables in the table after the join does not change. While mutating joins merged the tables creating a resulting table with more columns, with filtering joins we’re simply filtering the observations in one table based on the values in a second table. 5.0.7 How to Connect to a Database Online As mentioned briefly above, most often when working with databases, you won’t be downloading the entire database. Instead, you’ll connect to a server somewhere else where the data live and query the data (search for the parts you need) from R. For example, in this lesson we downloaded the entire chinook database, but only ended up using artists and albums. In the future, instead of downloading all the data, you’ll just connect to the database and work with the parts you need. This will require connecting to the database with host, user, and password. This information will be provided by the database’s owners, but the syntax for entering this information into R to connect to the database would look something like what you see here: con &lt;- DBI::dbConnect(RMySQL::MySQL(), host = &quot;database.host.com&quot;, user = &quot;janeeverydaydoe&quot;, password = rstudioapi::askForPassword(&quot;database_password&quot;) ) While not being discussed in detail here, it’s important to know that connecting to remote databases from R is possible and that this allows you to query the database without reading all the data from the database into R. 5.0.8 Summary In this lesson we discussed what relational data are and how to work with them in R. This led to a discussion about the package RSQLite, which allows you to connect to and make queries from databases. With relational data in hand, we discussed both mutating joins and filtering joins. Finally, we touched very briefly on how to connect to a remote database from R. 5.0.9 Additional Resources Relational Data - Chapter 13, by Hadley Wickham Tidy Animated Verbs, by Garrick Aden-Buie to visualize joins in relational data SQL databases and R, from Data Carpentry 5.0.10 Slides and Video Relational Databases Slides "],["unconventional-sources-of-data.html", "Chapter 6 Unconventional Sources of Data", " Chapter 6 Unconventional Sources of Data Up to this point in the course set, we’ve primarily worked with data in a tidy spreadsheet format. Whether this was data that we had to wrangle to get into that format or data stored in a database already in a tidy format, it was tidy nonetheless. It’s important, however, to know that data do not always come in a spreadsheet-friendly format. While as a data scientist you will often be working with data stored in spreadsheets, you’ll also likely work with some unconventional types of data. 6.0.1 Structured Data So far, we have primarily focused on structured data. Structured Data are data that have a high degree of organization. Tidy spreadsheets where the data can be easily analyzed using the data wrangling and visualization skills discussed so far in this course set are an example of structured data. 6.0.2 Unstructured Data Unstructured data, on the other hand, are data that are not organized in a pre-defined manner. These data do not fit into rows and columns in a spreadsheet. Unstructured data may contain text, images, audio files, and even video files. Given the differences in these types of data, traditional methods for cleaning and analyzing these kind of data are not as helpful. Nevertheless, there are ways to work with these data in R. We’ll introduce a number of different types of unstructured data and then give you a brief example of where you would start if you wanted to work with each unconventional data type in R. Unstructured data types 6.0.3 Text Data While most of the data we’ve seen so far has been stored in rows and columns within a spreadsheet, text documents or words of any kind can also be data! Text analytics or text mining is the process of taking large collections of text, generating a dataset from the that can be analyzed, and analyzing the words in that dataset. In other words, text mining is the process of converting textual data from unstructured form to a structured form for analysis or visualization. An application of text mining is sentiment analysis. In sentiment analysis, the goal is to categorize the text and quantify opinions expressed within the text. For instance, in a lot of satisfaction surveys, as you may have taken, the company asks you to express your opinion about your experience or a specific product in a few sentences. Historically, due to the fact that survey respondents could have typed anything they wanted in these boxes, these kind of data were often ignored when analyzing the survey data due to their unstructured nature. In other words, free text responses on a survey can be hard to analyze. In cases where these data were analyzed, the text from each survey respondent would have been read by a human. That human would asses what the response and assign a score as to how positive or negative the text was. However, using sentiment analysis we are now able to read a vast amount of textual data and use an algorithm to assign that value to respondent’s attitude toward the service or the product. There are different approaches to sentiment analysis. Sometimes, a paragraph of text will be evaluated to assess how sad or happy the words in the text are. Other times, you’ll use sentiment analysis to gauge how positive or negative the words in the text are. Other times still, words will be analyzed using sentiment analysis to determine how scientific or unscientific text is. For more on sentiment analysis read this this article. Sentiment analysis 6.0.3.1 Analyzing Text in R While not the most typical type of data analyzed in R, R can be used to analyze text. There are three packages that are particularly helpful for analyzing textual data sing text mining: tm : a text mining package tidytext : a tidy analysis package ; works with tidy data tools (i.e. dplyr) languageR : Statistical analysis of linguistic data To see an example of text analysis in R, check out David Robinson’s post Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half, where he uses the tidytext package, twitteR package, and Twitter API to analyze the tweets from Donald Trump during his presidential campaign. Analysis of tweets in R 6.0.4 JSON: JavaScript Object Notation In an earlier lesson in this course we touched on the fact that JSON is a text-based way to send information between a browser and a server and a frequent format in which you’ll retrieve data from an API call. We also mentioned that JSON format requires objects to be comprised of key-value pairs. For example, in the case of: {\"Name\": \"Isabela\"}, “Name” would be a key, “Isabela” would be a value, and together they would be a key-value pair. JSON data contain key-pairs That’s where we left our explanation of JSON data in the earlier lesson. Here we’ll use a snippet of JSON data to explain that in addition to using key-pairs JSON data are nested and hierarchical. This means that key-pairs can be organized into different levels (hierarchical) with some levels of information being stored within other levels (nested). Using a snippet of JSON data here, we see a portion of JSON data from Yelp explaining a restaurant. We’re looking at the attributes of this restaurant. Within attributes, there are four nested categories: Take-out, Wi-Fi, Drive-Thru, and Good For. In the hierarchy, attributes is at the top, while these four categories are within attributes. Within one of these attributes Good For, we see another level within the hierarchy. In this third level we see a number of other categories nested within Good For. This should give you a slightly better idea of how JSON data are structured. JSON data are hierarchical and nested 6.0.4.1 Analyzing JSON in R Above we discussed how to analyze pure text (meaning, text written by humans in their native written and spoken language). Here, we’ll discuss how to briefly how others have wrangled text-based data from the Internet in the JSON format within R. This is possible because of the R package jsonlite, which was used in the following example: Kan Nishida, a data scientist, was interested in understanding what restaurant types found most frequently in each state or province. To do this, he used JSON data originally released from Yelp. He wrangled the data from JSON format into a tabular format using jsonlite and other data wrangling packages, such as dplyr, to ultimately determine the types of restaurants found most frequently in a number of different states and provinces. Wrangling Yelp data from JSON into tidy tabular data 6.0.5 XML: Extensible Markup Language Extensible Markup Language (XML), is another human- and machine-readable language that is used frequently by web services and APIs. However, instead of being based on key-value pairs, XML relies on nodes, tags, and elements. The author defines these tags to specify what information is included in each element of the XML document and allows for elements to be nested within one another. The nodes define the hierarchical structure of the XML (which means that XML is hierarchical and nested like JSON)! XML format relies on nodes, tags, and elements XML accomplishes the same goal as JSON, but it just does it in a different format. Thus, the two formats are used for similar purposes – sharing information on the web; however, because the format in which they do this is different, a different R package is needed to process XML data. This packages is called xml2. 6.0.5.1 Analyzing XML in R To see an example of not only using xml2 to parse XML data, but also another example of using rvest to obtain the XML data, check out this post from José Roberto Ayala Solares where he took the text from a New York Times article called “Trump’s Lies”, scraped the data from the web (obtaining it in XML), and then wrangled it into a tidy format using xml2. rvest and xml2 are helpful for web scraping and working with XML data In this lesson, our goal is to make you aware that data from the Internet (and APIs in particular) will often come in either JSON or XML format. Thus, the JSON and XML examples provided here only give you a bit of an idea of what JSON and XML data are and how to work with them. Nevertheless, the more frequently you retrieve data from APIs and the Internet, the more comfortable you’ll have to become with both JSON and XML. And, jsonlite and xml2 will help you as you work with these data in R! 6.0.6 Images Only a few decades ago, analyzing a large dataset of images was not feasible for most researchers. Many didn’t even think of images as data. But, there is so much we can get from analyzing image data. Although we will not study images processing techniques in this lesson, let’s look at one example that give us an idea of how image data can be used. Within Google Maps there is a Street View feature that allows panoramic views from positions along many streets in the world. One of the things you may notice if you’re looking around on Google Maps’ street view is that for many streets in the world you do not only see houses; you are also able to see cars. Google Maps street view Some 50 million images of cars from over 200 cities were used by researchers to detect the make, model, body type, and age of the cars in each neighborhood. They were able to take unstructured image data and compile a structured data set! These same researchers then pulled together a structured dataset from the Census and the 2008 elections of demographic information (such as race and income), and voting history in these same neighborhoods. Data used from Google Maps street view to predict demographics of a neighborhood Using these two datasets (the Google Street view car image data and the demographic data), researchers used a technique known as machine learning to build an algorithm that could, from the images of cars in a neighborhood, predict the demographics (race, income, etc) and how that area is likely to vote. Comparing these two sets of data, they were able to accurately estimate income, race, education, and voting patterns at the zip code level from the Google Street view images. Cars in a neighborhood can predict how the area votes 6.0.6.1 Analyzing Images in R Like with text, there are packages in R that will help you carry out analysis of images. In particular, magick is particularly helpful for advanced image processing within R, allowing you to process, edit, and manipulate images within R. Like JSON and XML, where there is more than one file format for a similar task, there are also a number of different image file formats. We touched on the pros and cons of a number of these in the Data Visualization course; however, of importance here is the fact that the magick package is capable of working with many different types of images, including PNG, JPEG, and TIFF. The magick package has a particularly helpful vignette where you can learn the ins and outs of working with images using magick’s functionality. Their documentation will discuss how to read image data into R, how to edit images, and even how to add images to your R plots! [magick package’s example of adding an image to a plot]( 6.0.7 Audio Files So much of the data generated today comes in the form of audio files. This could be all the data contained within the MP3 files of your favorite songs or the audio files saved in the archives from the speeches of politicians. While certainly not tabular data, there is a lot of information stored within all the audio files we’ve generated! It likely isn’t surprising that there are many different file types to store audio data. MP3s are currently the most common file format, becoming popular as CDs became less popular due to the fact that the same audio file saved as an MP3 would take up much less space than the same file stored on a CD. However, if working with audio files, one would want to learn much more about how audio files are stored in different file formats. 6.0.7.1 Analyzing Audio Files in R There are two packages that will likely be helpful for analyzing audio files in R: tuneR - package to analyze music and speech seewave - package to analyze, manipulate, display, edit and synthesize sound waves Admittedly, working with audio files in R is not as common as working with tabular data; however, that just means there’s room for development and exploration of how to approach audio files in R. To see how others have worked with audio files in R, take a look through the following examples: transcribing music in R using tuneR, by Vessy clustering music in R using tuneR and seewave, by Vessy 6.0.8 Summary In this lesson, we’ve briefly reviewed a number of different unconventional sources of data. These are data sources that are unstructured and not tabular. We’ve provided examples for each type of unconventional data and links to where you may start if you wanted to work with each type of data in R. This lesson is an introduction into these types of data, but is by no means exhaustive. To get more practice, it’s best to complete the exercise accompanying this lesson, where you’ll get practice working with the tidytext package and sentiment analysis! 6.0.9 Additional Resources 6.0.9.1 Text Mining tm vignette “Text Mining with R”, by Julia Silge and David Robinson Analysis of Trump’s tweets, by David Robinson tutorial: “Text Mining in R””, by Shubham Simar Tomar 6.0.9.2 JSON &amp; XML JSON Yelp Tutorial, by Kan Nishida An Introduction to XML xml2 package xml2 Trump’s Lies example 6.0.9.3 Images magick package “Google Street View Can Reveal How Your Neighborhood Votes”; Original Publication 6.0.9.4 Audio tuneR and seewave tutorial, by Sam Carcagno tuneR tutorial: transcribing music, by Vessy tuneR and seewave tutorial: clustering music, by Vessy 6.0.10 Slides and Video Unconventional Sources of Data Slides "],["finding-data.html", "Chapter 7 Finding Data", " Chapter 7 Finding Data Now that we know what data are, how to work with them in RStudio Cloud, and how to get them into RStudio Cloud, if you have a question you want to answer with data, where do you find data to work with? In some cases you’ll have to create your own data set but in other cases you can find data that others have already generated and start from there! In this lesson, we’ll discuss the difference between public and private data and direct you to a number of resources where you can find helpful data sets for data science projects! 7.0.1 Public versus Private Data Before discussing where to find data, we need to know the difference between private and public data. Private data are datasets to which a limited number of people or groups have access. There are many reasons why a dataset may remain private. If the dataset has personally-identifiable information within it (addresses, phone numbers, etc.), then the dataset may remain private for privacy reasons. Or, if the dataset has been generated by a company, they may hang onto it so that they have an advantage over their competitors. Often, you will not not have access to private data (although sometimes you can request and gain access to the data or pay for the data to get access). But that’s OK because, in general, public data are freely-available. Unlike private data generated by companies, data generated by governments are often made public and are available to anyone for use. 7.0.2 Publicly-available data As a data scientist, there’s a good chance you may work with private company data as part of your job. However, before you have that job, it’s great practice to work with datasets that are publicly-available and waiting for you to use them! In this section, we’ll direct you to sources of different datasets where you can find a dataset of interest to you and get working with it! 7.0.2.1 Open Datasets There are a number of companies dedicated to compiling datasets into a central location and making these data easy to access. Two of the most popular are Kaggle and data.world. On each site, you’ll have to register for a free account. After registering you’ll have access to many different types of datasets! Explore what’s available there and then start playing around with a dataset that interests you! kaggle and data.world are great places to look for datasets Publicly-available datasets are also curated at Awesome Public Datasets, so feel free to look around there as well! 7.0.2.2 Government Data Government data can provide a wealth of information to a data science. Government data sets cover topics from education and student loan debt to climate and weather. They include business and finance datasets as well as law and agriculture data. Here we provide lists of governments’ open data to just give you and idea of how many datasets are out there. This will only include a tiny portion of what cities and federal governments’ data are available for you to use. So, if there’s a place whose data you want to work with, look on Google for “open data” from that place! 7.0.2.2.1 US Data If you’re interested in working with government data from the United States, data.gov is place to get datasets that have been released by the the United States government. Here you can find hundreds of thousands of datasets. These data cover many topics, so if you’re interested in working with government data, data.gov datasets is a great place to start! data.gov has hundreds of thousands of datasets 7.0.2.2.2 Census Data The US Census is responsible for collecting data about the people within the United States and United States’ economy every ten years. These data are also accessible online and they can be worked with in R using the very helpful tidycensus package! The US Census provides data about the US people and economy 7.0.2.2.3 Open City Data The US’s federal government is of course not the only place to obtain government data. More and more cities across the world are starting to release open data at the city level. A few of these cities and their respective open city data links are provided below: Baltimore, MD (USA) Cincinnati, OH (USA) Las Vegas, NV (USA) New York City, NY (USA) San Francisco, CA (USA) Toronto, Ontario (Canada) Additionally, to see a summary of what datasets are available from cities across the USA, check out the US Open City Data Census from the Sunlight Foundation. US City Open Data Census 7.0.2.2.4 Global Data In addition to the United States, there are many other countries providing access to open data with more and more providing access and updated datasets each year. These include (but are not limited to!) datasets from many countries within Africa and Latin America as well as Canada, Ireland, Japan, Taiwan, and the UK. Additionally, to see what datasets are available globally, the Global Open Data Index is a great place to start! Global Open Data Index 7.0.2.3 APIs We’ve mentioned APIs previously, but it’s important to include them here as well. APIs provide access to data you’re interested in obtaining from websites. There are APIs for so many of the websites you access regularly. Google, Twitter, Facebook, and GitHub (among many others) all have APIs that you can access to obtain the dataset you’re interested in working with! 7.0.2.4 Company Data Finally, we mentioned above that companies often keep their data private for a number of reasons, and that’s ok! When companies do release their data, they will often be found on websites like Kaggle and data.world. If there is a company whose data you’re interested in, you can search for the company’s data on either of these two data repositories or on on the company’s website directly to see if they provide the data there or if you can scrape their website to obtain the information you need! There may not always be a way to get the exact dataset you’re looking for, but you can often find something that will work! 7.0.3 Data You Already Have Sometimes, it’s not about finding data someone else has already collected on a bunch of individuals in a population. Rather, getting data sometimes just involves taking a look at things you already have but just haven’t yet realized are data you can analyze. For example, MP4 files you’ve bought and have on your computer are data! They can be analyzed using tuneR and seewave. You could use this type of data to categorize the music in your library or to build a model that takes data on what songs were already big hits to determine what qualities of a song predict that it may be a big hit. Alternatively, you could scrape the websites you frequently visit (using rvest!) to answer interesting questions. For example, if you were interested in writing a really great title for the newest video of your pet doing something super cute, you might scrape the web for titles of pet videos that have recently gone viral. You could then craft the perfect title to use when you upload your pet video. Granted, this may not be an example answering the most important type of data science question; however, writing up how you did this would make a really great blog post, which is something we’ll discuss in a lesson in a few courses! Finally, social networking websites like Facebook and Twitter, collect a lot of data about you as an individual. You have access to this information through the websites APIs, but can also download data directly. After news of the Facebook and Cambridge Analytica data breach, many articles were published about how to download your Facebook data. These data can be downloaded and then analyzed to look at trends in your data over time. How many pictures have you uploaded and been tagged in over time - has that changed? What topics do you most frequently discuss in Messenger? Or, maybe you’re interested in mapping the places you’ve been based on where you’ve checked in. All of these data can be analyzed from data that are already there, just waiting for you to work with them! In all, sometimes getting the data just means realizing the data you already have at your disposal, figuring how to get the data into a format you can use, and then working with the data using the tools you have! 7.0.4 Summary In this lesson, our goal was to give you an idea of where to find data so that you can start working on interesting data science projects. Once you’ve located an interesting dataset, use the skills learned throughout this course to get the data into R. Then, get wrangling! Before you know it you’ll be more than halfway through an interesting data science project. Often finding and wrangling the data take up the most time! 7.0.5 Slides and Video Finding Data Slides "],["internet-safety.html", "Chapter 8 Internet Safety", " Chapter 8 Internet Safety Just like driving a car, there are inherent risks to using the Internet and doing data analysis on the cloud. These risks shouldn’t mean that you should always be fearful about your online safety, however, they should make you more aware that there are consequences if you are not cautious. Internet safety (or online safety) revolves around being knowledgeable about what these risks are, knowing how to deal with risks that arise when working online, and working hard to increase personal safety against security risks associated with using the Internet. The goal of Internet safety is to maximize personal safety and minimize security risks. We’ll discuss the basics of this below, but know that there are people whose entire careers are dedicated to Internet safety, so this will simply touch on the basics. 8.0.1 WiFi WiFi is the technology that enables you to wirelessly connect to the Internet. Computers, smartphones, video-game consoles, and TVs are among the many types of devices that are WiFi-compatible. As is often the case, increased access to the Internet comes with increased risk to network security. 8.0.1.1 Public WiFi Completely-open and free public WiFi, or Internet access that does not require a password or a screen to login from is the least secure type of network. When connected to public WiFi, be extra vigilant. Avoid working with sensitive data while on a completely open and public network, and do not make online purchases while on this network. 8.0.1.2 Semi-Open WiFi Semi-open WiFi networks are networks potentially open to everyone, but that may require a password (maybe one that is printed on your receipt at a coffee shop) or may require you to login by providing an e-mail address. When given the choice, opt to connect to these networks rather than completely-open public WiFi networks. 8.0.1.3 Password-protected WiFi The most secure option, however, is when you connect to a password-protected WiFi network, so use password-protected WiFi whenever you have the option. WiFi at your home should absolutely be password-protected. 8.0.1.4 Use HTTPS Whenever Possible The “S” in HTTPS stands for secure. So, what does HTTP stand for? HTTP refers to “Hyper Text Transfer Protocol.” You’re likely most familiar with these letters as being part of your website. When you type “www.gmail.com” into your Chrome browser, Chrome redirects you to “https://mail.google.com/mail/u/0/#inbox.” HTTP refers to how the data are sent from your browser (where you typed “www.gmail.com”) and the website to which you’re trying to connect (www.gmail.com). When the web address starts with https:// rather than http://, this means that the data sent over your Internet connection are encrypted. Encryption is a process that generates a message that cannot be decoded by anyone without a key. Thus, if someone were to intercept your encrypted data as it were transferred over the Internet, they may be able to intercept your data, but they wouldn’t be able to decipher the information, as they wouldn’t have access to the key. When a website is using encryption (https://), there will be a padlock to the left of your web address bar to indicate that the HTTPS connection is in effect. When sharing sensitive information on the Internet (ie a credit card number or personal health information), it should only ever be done on a secure (encrypted) network. 8.0.2 Passwords In the age of apps for everything and technology being everywhere, you likely have a lot of requests daily for passwords. While they may seem like an annoyance, using passwords, and importantly, using good passwords is worth the effort. 8.0.2.1 Strong Passwords Strong passwords may be harder to remember, but that’s the point. You don’t want someone else to be able to easily guess your password. Characteristics of Strong Passwords: Use a combination of random letters Use both capital and lower case letters Include numbers, letters, and symbols Disperse the letters and symbols throughout the password (not just at the beginning and the end) Has at least 12 characters What to Avoid when Creating A Password: Avoid using names of you or anyone in your family Do not include the words “Passcode” or “Password” Avoid using sequential numbers (i.e. 123) Avoid using your telephone number Don’t make obvious substitutions to words (i.e. avoid simply replacing the letter “o” in a word with the number “0”) 8.0.2.2 Utilize Passwords When given the option, use a password. Do not opt out of using a password to log in to your phone or your computer. When asked to set a password, do so, and make it a good one. In addition to setting passwords, make sure you have multiple different passwords. Do not use the same password for all your most important accounts. If someone were to log onto your WiFi network, you wouldn’t also want that person to be able to gain access to your credit card, Gmail, bank account, and/or Facebook account. Use different passwords for different accounts. 8.0.2.3 When to Change Passwords If your password was set by a third-party company (say your Internet provider), you should change your password right away. Otherwise, it’s a good practice to change passwords to your most sensitive information (i.e. WiFi, credit cards/banking, etc.) at least every six months. 8.0.2.4 Never Share Passwords A final word on passwords: never share them. Reputable companies will never ask for them. Your bank will never require you to tell it to them. And, neither will the IRS. If someone is asking for your password, it is almost certainly a scam. 8.0.3 Good Internet behavior On the Internet there are few basic guidelines to follow that will help you be a good citizen and help keep you safe. This list is surely not exhaustive, but it’s a good start. When online: * Don’t be a jerk * Never share your passwords * Think before you click * Don’t click on links sent to you from people you don’t know * Don’t click on links from someone you do know if it doesn’t seem like something they would send (i.e. an email with a weird subject line that doesn’t sound like them or a link in an email that says “Vacation Pix” when they wouldn’t normally send those pictures, wouldn’t use the word “Pix”, or haven’t recently been on a trip) 8.0.4 Online Scams While we would love if everyone on the Internet behaved well (and most people do!), there are of course bad actors. To avoid getting caught up in an online scam, beware of: people/websites posing as people from a trustworthy company who attempt to obtain your private information - emails asking for credit card numbers or passwords are scams people who call and ask for passwords to accounts and who seem to know a lot about your family and claim to be from law enforcement. This is likely a scam. You can always hang up and call your local law enforcement directly to be sure. people/websites offering prices that are way too low for housing or big purchases on the Internet. These are likely a scam. 8.0.5 Malware &amp; Spyware Finally, malware and spyware are software designed to be malicious. The goal of malware and spyware is to collect your private information (usernames, passwords, credit card numbers) without you ever knowing. This software often acts through e-mail or other software. By avoiding clicking on suspicious links and not downloading software with which you’re not familiar, you can avoid the issues caused by malicious software. 8.0.6 Security on a Chromebook Because Chromebooks do not run a typically operating system, they are much more secure than other laptops. While explained in greater depth on Google’s support documentation, Chromebooks have certain advantages over other laptops when it comes to security: Automatic updates - ensure that your Chromebook is never behind on security updates Sandboxing - each web page runs in its own environment (sandbox), so if one page is infected, that page won’t affect other tabs or apps running at the time. Verified Boot - every time the Chromebook starts up, it does a self-check to detect any issues with security. Encryption - web apps on a Chromebook store all data safely in the cloud with encryption, making it resistant to tampering 8.0.7 Slides and Video Internet Safety Slides "],["data-privacy.html", "Chapter 9 Data Privacy", " Chapter 9 Data Privacy In the previous lesson, we covered good practices when connecting to WiFi and working on the Internet. We noted that data are safer when they’re encrypted, but we didn’t go into much more detail than that. In this lesson we’ll cover what data privacy is, why it’s important, and discuss encryption in slightly more detail. Again, remember that there are people who dedicate their entire careers to data privacy. This lesson simply touches on the basics. Feel free to search on the Internet to learn even more about Data Privacy! 9.0.1 What is Data Privacy? Data privacy is: {cite: “Wikipedia”, url: “https://en.wikipedia.org/wiki/Information_privacy”} &gt; the “relationship between the collection and dissemination of data, technology, the public expectation of privacy, and the legal and political issues surrounding them.” This complex definition correctly suggests that data privacy is not a simple matter. A simpler definition of data privacy would maybe be how to keep personal and private information safe. Concerns arise with data privacy whenever there is personally identifiable information (PII) or sensitive information that is being collected, stored, used, (and eventually destroyed and deleted). When handled appropriately, only people who should have access to the information do. When mishandled, this information could be accessed by others who should not have that information. Data are everywhere, so it’s incredibly important to think about data privacy early on in every single data science project. It’s much better to be too vigilant than to make a mistake, allow data to fall into the wrong hands, and have to try to clean up the mess, as you’ll see in a few examples below. So, think of data privacy concerns early and often to avoid making a mistake. 9.0.2 Personally Identifiable Information In the US, Personally Identifiable Information, often simply referred to by the terms initials, PII, is any “information that can be used on its own or with other information to identify, contact, or locate a single person, or to identify an individual. This could be a first and last name, a picture of someone’s face, a driver’s license number, a date of birth, an address, an individual’s genetic code, or a telephone number, among many other pieces of information. All of these pieces of information could help identify a person. It’s these type of data that must be handled extremely carefully. When you sign up for an app on your computer or phone, you likely provide that company with some PII. You often give them your name and your address (among other information). You likely assume that this company will handle your data appropriately, and, if everyone is following the rules, they will. This means they won’t share it with others, use it for purposes other than those for which you’ve given them permission, or allow others companies/people to steal your information. However, as reported regularly in the news, companies do not always handle your data as they should. We’ll discuss what concerns you should have as you’re working with other people’s data and how data should be handled. 9.0.3 What is encryption? To review from the last lesson, when data are encrypted, all the information is jumbled into a code that can only be decoded by someone who has the key. This means that if data are intercepted while being transferred, the person who has intercepted the data will not be able to understand the data, as they won’t have access to the key and will only have the completely jumbled information. It is a security measure to help keep data safe. 9.0.3.1 Working in RStudio Cloud In earlier lessons, you’ve been working in RStudio Cloud and will continue to do so as your hone your data science skills. Data on RStudio Cloud are encrypted at rest (when they’re sitting in your project) and in transit (when they’re being uploaded or downloaded). This means that, if intercepted, the person who has intercepted the data will not be able to decipher the information contained, as they don’t have the key. However, if you download data from RStudio Cloud and store them directly on your computer, they are no longer encrypted. If you have PII in a dataset that is stored on your laptop directly, that information is not protected. So, be careful working with datasets that contain PII that have been downloaded to your computer. 9.0.3.2 HTTPS An additional reminder: websites that have https:// (rather than just http://) also use encryption for data transfer, increasing security for data that are transferred. When working with PII and transferring data, be sure that the websites you’re working with are using HTTPS. 9.0.4 Human Security Human security is the concept that individual people (rather than just countries) should be kept safe and secure. With regards to data science, this means that when working on the Internet, regulations and laws should be made not just to protect the security of a nation, but rather to protect every individual and their data. Data Science projects should NOT: * increase risk or harm * pose threats to individuals * make individuals’ PII publicly-available or susceptible to theft * break any laws * share data with other groups/companies without the consent of the individuals * saved data insecurely 9.0.5 Computer Security Computer Security (or cybersecurity) is {cite: “Wikipedia”, url: “https://en.wikipedia.org/wiki/Computer_security”} &gt; “the protection of computer systems from the theft and damage to their hardware, software or information, as well as from disruption or misdirection of the services they provide.” This means that, in addition to keeping individuals’ data safe (maximizing human security), data science projects must also consider how to ensure that the computers and software they use and create are designed in a secure manner. For data science projects, this means that data being transferred over the Internet should be done so safely (using encryption) and that software you develop or use should not post PII or other sensitive to the Internet. 9.0.6 Open Science There is an important movement in science currently where individuals promote what is known as open science. Practitioners of open science support making their scientific studies as transparent as possible. Specifically, they provide all the data, programming code, and software necessary to repeat their analyses. Open science is great for the scientific community and public, as everyone has access to the tools and data used. However, it is not without its privacy risk. Open Science practitioners are still responsible for protecting PII. This means that individual-level information (names, dates of birth, addresses, etc.) must all be removed from one’s dataset before it is released to the public. Being a supporter and practitioner of open science is great, but you must always ensure that when you’re releasing and sharing data, you are not inadvertently putting others at risk. 9.0.6.1 Open Data Open data are data that are freely-available to anyone. Local and federal governments as well as many scientists make their data freely-available to the public. Open data are wonderful in that anyone can work with them. Sharing data is a great thing! But, sharing must be done with the caveats already mentioned. Individual-level information (PII) should be removed from datasets prior to release. Individuals should not be identifiable from data freely-available on the Internet. 9.0.6.2 Open-source Software Like open data, open-source software are software that are designed to be freely-available to anyone. This means that the software code is all available, so others can not only download and use the software for free, but also can download the code and modify it for their own purposes. The R programming language is itself an open-source project and all packages made for R are also open-source. This means that programming in R will always be free, and the code will always be publicly-available. 9.0.7 Data Breaches To understand data privacy, it’s good to know definitions and best practices, but it’s also important to learn from past mistakes. Companies have not always handled data securely. Here, we’ll discuss a few famous data breaches and touch on what went wrong so these errors can be avoided in the future. 9.0.7.1 Facebook In March of 2018, The New York Times and The Guardian both broke stories about how data from at least 50 million Facebook profiles were used by a company called Cambridge Analytica to build profiles of individual US voters, allowing individuals be to targeted with personalized political advertisements. While the details are provided in the links provided above, briefly here, the data were mined from individuals who had consented for their data to be used for academic purposes. However, Cambridge Analytica was collecting these data for non-academic purposes and collecting data from friends of these individuals who had consented to have their data collected but who had not consented to have their data used. This data breach at Facebook demonstrated that Facebook did not have sufficient protocols in place to protect user data. As a result, Mark Zuckerberg, Facebook’s CEO, has had to testify before Congress and the company has promised to make changes to improve data security. 9.0.7.2 Equifax In the Fall of 2017 Equifax, a credit-reporting agency, disclosed that partial driver’s license information was stolen from 2.4 million US consumers. This hack was traced back to a “preventable software flaw”. In other words, in March of 2017, Equifax identified a weakness in a piece of software they were using. However, they did not move to fix that weakness quickly. Partial drivers licenses were stolen from May until June of 2017. Thus, Equifax knew about the problem before any data were actually breached. The entire breach could have been avoided. Prioritizing security and moving to fix problems immediately upon realizing an issue is critically important. 9.0.7.3 Ashley Madison In July of 2015, Ashley Madison, a website that helps people cheat on their spouses, was hacked and personal information, including e-mail addresses, from 32 million site member’s were stolen and published. The windfall rippled through society, leading to resignations, divorces and suicides. The “Impact Team,” the group of hackers responsible for this hack, publicly stated that part of the reason they decided to release the data was because Ashley Madison had been charging users a $19 fee to completely erase their profile information, but that Ashley Madison had failed to actually scrub these data from their records. The security lesson here is that there should always be a mechanism by which users can remove themselves from a dataset. In the European Union, the General Data Protection Regulation (GDPR) has stated that, by law, companies must have this feature built-in. Users should always be able to remove themselves from a dataset. 9.0.7.4 OKCupid In May of 2016, researchers scraped profiles from 70,000 users on OKCupid, an online dating site, and made them completely open to the public. These researchers did not (1) have permission from OKCupid, (2) obtain informed consent from the users, nor did they (3) remove PII from the data prior to release. Since their original release on The Open Science Framework, given the issues with the dataset just mentioned and the fact that the collection of the data did not follow OKCupid’s Terms of Service, the dataset was taken down. However, hundreds of people had already downloaded the data at that point. In this case, a set of researchers did not comply with the Terms of Service they agreed to by signing up for OKCupid. While it was likely not illegal to obtain the data, as OKCupid made the data sort-of-public, it was certainly unethical, as the researchers did not take individual data security practices into consideration. So, even when you’re not necessarily doing something illegal, you can still do something unethical that does not follow good data privacy practices. 9.0.7.5 Strava A similar example of using data legally but with unintended negative consequences occurred when data from Strava, a popular fitness-tracking app, gave away the location of secret US army bases throughout the world in late 2017. Individual users would use the app to track their activity; however, in foreign countries, where Strava-users are almost exclusively foreign military personnel, bases can be identified easily from Strava heatmaps, which show all the activity tracked by its app users, as they are the only individuals contributing to the maps in those areas. The lesson to be learned here is that when releasing data, always consider possible unintended consequences. Think hard about the implications of your data and your analyses. Being vigilant and always considering data privacy and possible unintended consequences of your work are incredibly important. 9.0.8 Conclusions Hopefully we’ve explained a number of considerations to take into account when working with and sharing data. Data are powerful. It’s important to use them responsibly. Be careful with PII Fix security issues immediately Keep individuals’ data safe Don’t steal data Even if it’s legal, it may not be right or ethical Consider unintended consequences of your work 9.0.9 Slides and Video Data Privacy Slides "],["ethical-data-science.html", "Chapter 10 Ethical Data Science", " Chapter 10 Ethical Data Science From the emails automatically marked as spam in your Inbox and facial recognition on Facebook to the targeted advertisements you see whenever you’re online, data science has made its way into our everyday lives. Often, this is a positive because data scientists generally set out to improve the world (say, by analyzing crime data in hopes of reducing violent crime) or to make our lives easier (by preventing spams in our mailboxes, for example). However, data science projects are certainly not without negative consequences. We discussed some of the companies who have run up against issues properly securing user data in the last lesson; however, protecting the privacy of users’ data is not the only time data scientists should consider the ethics of their work. In fact, ethical considerations should be made from the start of a project and should never stop being made. We’ll discuss what we mean by that in this lesson. When we talk about the ethics of data science, we’re talking about how to use data to improve the lives of individuals in our society in the world without causing harm to any groups with our work. We’re talking about putting in the work ahead of time to avoid unintended negative consequences of our work. Rather than acting surprised when groups are inadvertently harmed or only dealing with unintended consequences after they occur and harm others, an ethical data scientist will consider whether they are doing any harm at each step along the way. 10.0.1 Data Science Team Data science projects can be done by an individual; however, more often, they are carried out by a team of individuals who are all interested in using data to answer an interesting question. Whether it’s one person setting out to answer the question or a team working to answer this question, it’s important at this outset of a project to really think about whether all the necessary individuals are on the team. For example, if a group of data scientists were setting out to reduce crime in a neighborhood, it would be in their best interest to either be very familiar with that neighborhood (say, live there themselves) or to talk to individuals who currently live in that neighborhood to be as informed as possible before they set out to analyze their data. By working with individuals who have more knowledge about the neighborhood than someone who has never been there, the team will be less likely to make incorrect assumptions or leave out critical pieces of information when analyzing the neighborhood’s data. 10.0.2 The Data After ensuring that the team you’ve assembled to answer the question is the right team to do so, there are many considerations to be made when collecting data. 10.0.2.1 Sampling Bias When data are collected, it’s difficult to ever get information about an entire population. Thus, when data are conducted, researchers will typically get data from a subset of individuals within the population (a process called sampling) and then infer something about the entire population using what they learned from the subset of individuals. They will try to ensure that their subset is a random subset of the population. However, whenever sampling of a population is done, there’s a chance for sampling bias. Sampling bias occurs whenever the sample of the population collected does not accurately reflect the population as a whole. For example, if the population you’re interested in studying is half female and half male, but the subset of people you’ve sampled is made up of 70% females and 30% males, then your sample is biased. Having a plan to avoid sampling bias is an important first step in the process of data collection. Checking the data after the data have been collected to ensure that the sample is not biased is an equally important step. When data are collected by survey for example, they may be sent out to an equal number of males and females; however, maybe more males respond than females. Even though your plan was solid, your responses are biased. This has to be accounted for during the analysis, as your responses do not accurately represent the population as a whole. And to be clear, gender is not the only consideration to make when sampling. Samples can be biased by many factors including (but not limited to) race and ethnicity, age, income, and education. Sampling bias should always be avoided both when planning to collect data and after the data are collected. 10.0.2.2 Informed Consent In addition to collecting data in an unbiased manner, data must be collected ethically. This means that individuals must consent to participating in data collection. Informed consent requires that the individual agreeing to share their information knows what data are being collected, has been informed of any possible consequences, and has been provided full knowledge of any risks or benefits of participation. If data are collected on individuals and later on a data scientist wants to use those data for a different purpose, they can only do so if the initial consent allowed for the data to be used in this manner. Alternatively, if the data are de-identified, they may be able to be used for future analysis; however, this must be done with care. 10.0.2.3 Privacy While discussed in a previous lesson, for completion’s sake, we’ll discuss again here. When collecting personally-identifiable information, these data must be protected. They should not be shared without the participants’ consent and they should be stored in a secure manner. 10.0.2.4 Withdrawal of consent If an individual consents to their data being collected and later changes their mind, they have every right to do so. It is your responsibility as the person collecting the data to have an effective plan for removing any data already collected from that individual. 10.0.3 The Analysis After avoiding sampling bias, collecting data ethically, avoiding privacy issues in data storage, there are still a number of ethical considerations to take into account during ones analysis. While we don’t discuss the details in these courses, machine learning is currently a popular way to analyze data. In general, it uses existing data to predict how individuals will behave in the future. This means that the any biases in the data used to build the model (the existing data) will likely be perpetuated in the predictions. This means that, for example, in predictive policing, where data scientists are trying to predict where crimes are more likely to happen, if the data used to build the model (often referred to as an algorithm) come from crimes that have occurred in primarily black neighborhoods, they are going to predict that crime happens more in primarily black neighborhoods. Then, if police officers are placed in these neighborhoods, they are going to find more crime, not because crime has increased, but just because there are more of them in the neighborhood now. Essentially, any biases in the data used initially are going to be perpetuated in the models they generate. This means that it is critically important that data scientists fully know and understand the data their working with, its biases, and its limitations. Further, even when something like race is predictive in the model, it’s important for a data scientist to consider whether or not that variable should be included. Are you absolutely positive that race is the important variable and it’s not just that you have a biased sample? Instead, if you’re trying to predict crimes, it’s likely best to use data about crimes that have occurred and where they have occurred in the model rather than race, since race is not a measure of what you’re interested in modeling. Considering the implications of ones work when carrying out an analysis is incredibly important, and making decisions that do not unnecessarily harm any group of people is critical It was previously common for analysts to say that “data do not lie” and “algorithms are impartial,” but recent history has shown that that is simply not the case. Data are collected by humans, and humans are not without their biases. The best way to avoid bias in your data is to be aware of this and constantly check to make sure you’re not disadvantaging groups of people with your analyses. Before considering any data analysis and publishing your results, make sure: You have gone through your data and have made sure there is not any major flaw in your data collection. Data can be biased. You can’t use the result of a study that is mostly based on data from men to say anything about women. You have checked for common mistakes in your analysis and know that your methodology is valid and defensible. Messing up the data for a single variable can drastically change the result of your analysis. The results of your work can not be used to harass people, especially minorities, in any way. Your analysis is independent of your opinion about the specific problem you’re trying to solve using data. When carrying out an analysis, you should be looking for the answer to a question, but be careful not to want a specific answer. By wanting the analysis to go a certain way, you can subconsciously analyze the data to get that answer. It’s best just to collect the data ethically and analyze it carefully. Then, the answer is whatever the answer from the analysis says it is. You can do awesome things with your data science skills: cure diseases, analyze health data, prevent climate change, improve your city, or fact check politicians. Don’t let your biases or mistakes get in the way. 10.0.4 After the Algorithm Finally, after someone answers an awesome data science question with their really great analysis and tell the world, they often think their job is done. However, if you’ve designed an algorithm that is going to predict something going forward or that is going to continue to be used, it is your job to maintain that algorithm. That means that it’s your job to check for biases after the fact. Is your analysis disadvantaging groups of people? If it is, should it be? Does something need to be changed. There has to be a way for you to update your algorithm going forward. If something is continuing to be used by others, your job isn’t done once you’ve built the algorithm. It’s your job to check for biases after the fact and to update your algorithm should there be a need. 10.0.4.1 Ethical Questions to Answer To summarize this lesson up to this point, be sure to answer all of these questions for every data science project you carry out: Does the team on this project include all the necessary individuals to avoid bias in our analysis? Does our data collection plan address ways to avoid sampling bias? Are the data we’ve collected / the data we’re using to answer the question of interest biased? In what ways? Has informed consent been obtained from all participants? Do we have a mechanism do remove an individual’s data from our dataset if they so choose? Are the variables we’ve chosen to use for our analysis appropriate? Do they discriminate against anyone? Is our analysis transparent? Do we understand how and why we’re getting the answer we’re getting? Have we considered possible negative or unintended consequences of our analysis and its results? Do we have a way to update our analysis/algorithm going forward should biases in the results be found? 10.0.5 Ethics in Data Science Now that we’ve discussed the ethical considerations to be made before and throughout every data science project, we’ll discuss a few data science projects that were recently covered the popular media due to the questionable ethics of the project. We present these examples not only to highlight the ethics of the particular project, but also to state the importance of considering the implications of your work. It is not enough to argue that you just “did the analysis” but “didn’t think of the implications” of the work. As a data scientist it is your responsibility to both do the analysis and consider the implications of your work. 10.0.5.1 Data Science in Sentencing In April of 2017, Wired reported in an opinion piece that courts were using artificial intelligence to sentence criminals and made the claim that such practices must be put to an end. The piece explained that courts and corrections departments across the United States collect data on defendants that are then used in an algorithm to determine the “risk” that defendant poses. This aim of this algorithm is to determine how likely it is that the individual will commit another crime. These likelihoods of reoffending from the algorithm are then used to make decisions about how to set bail, what the sentence should be, and the details of that individual’s parole. These tools are often built by private companies. This means that exactly how they work and what data are used to assess risk are not disclosed. The article in Wired highlights the use of Compas, one of these risk-assessment tools developed by a private company and used by the Department of Corrections in Wisconsin, in a judge’s decision to give a man a long sentence, in part because of his “high risk” score from the algorithm. In a courtroom, however, because the specifics of how this algorithm works are not disclosed, a judge and jury would not be able to determine whether or not the likelihood of reoffending was calculated accurately by the algorithm. Initially, arguments were made that removing human biases and judgment from bail, sentencing, and parole decisions would be for the benefit of society. This initial goal was a noble goal that would improve society. However, over time, biases in the data used to generate these algorithms that perpetuated into biases in the algorithms’ outcomes, the lack of transparency in how they work, and failure to check for inequities and biases in the algorithms after they were put in place have led to unfair decisions being made. While individuals may have been looking to improve society with their algorithm at the outset of this project (or at least that argument can be made), lack of considerations of the implications of their work and lack of transparency in the process of generating and using the algorithm have led to questionable ethics in the use of algorithms to make sentencing decisions. 10.0.6 Artificial Intelligence in the Military In March of 2018, Gizmodo reported that Google was helping the US Department of Defense use artificial intelligence to analyze drone footage, a project that started in April of 2017. The project between Google and the Defense department is called Project Maven, and the goal of this project was initially to take drone footage and accurately identify objects (such as vehicles) and to track individuals in the footage collected. A number of Google employees were upset that Google would provide resources to the military that would be used for surveillance in drone operations. Due to the fact that in other non-military situations the use of machine learning has led to biased outcomes, other employees felt there needed to be further discussion about the ethics in developing and using machine learning technology before it was deployed by the military. In response to these concerns, a Google spokesperson stated that Google is working “to develop policies and safeguards”, that the technology is being used for “non-offensive uses only.” In this and many large data science projects where machine learning and artificial intelligence are being used in situations where lives could be negatively impacted, the project would benefit from taking ethical considerations into account and discussing these in a transparent way before deploying the technology. Developing policies and safeguards after the fact is not enough. 10.0.6.1 Facial Recognition in Law Enforcement In May of 2018, the ACLU reported that Amazon had teamed up with law enforcement agencies to use Amazon’s face recognition technology, Rekognition. The ACLU called this new technology, which can quickly scan images to “identify, track, and analyze people in real time” both “powerful” and “dangerous.” This story was picked up by many news sources, including a piece in the New York Times Proponents of using facial recognition technology in law enforcement cite that such technology can help locate and identify criminals more quickly than we would have been able to in the past and that it will help “keep residents and visitors…safe”. Proponents also argue that those who use the technology do so within the limits of the law. Opponents, like the ACLU, however, cite the civil liberties and civil rights concerns that constant surveillance with using facial recognition technology pose. The ACLU argues that there is substantial “capacity for abuse” and due to this, that citizens should not be watched by the government any time they walk outside, that facial recognition systems threaten freedom, and that the deployment of these technologies post a greater threat to communities that are already unfairly targeted by the political climate. Concerns with the technology cite that anyone can be identified, not just criminals, a problem for civil liberties. Opponents further question the accuracy of the technology. Inaccuracies, which would misidentify individuals and suggest they were guilty of a crime when they were in fact not is clearly a problem. Further, there are concerns that accuracy varies based on gender and race, which if true, poses a clear bias in the use of this technology for the identification of criminals as certain groups of individuals would be misidentified more frequently than others. 10.0.7 Additional Resources Ethical Checklist for Data Science, by Alan Fritzler Weapons of Math Destruction, by Cathy O’Neil Automating Inequality, by Virginia Eubanks Courts are Using AI to Sentence Criminals. That must Stop Now, by Jason Tashea in Wired Machine Bias, by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner at ProPublica Amazon Teams Up With Law Enforcement to Deploy Dangerous New Face Recognition Technology, by Matt Cagle and Nicole A. Ozer with the ACLU Amazon Pushes Facial Recognition to Police. Critics See Surveillance Risk., by Nick Wingfield at the New York Times Google Is Helping the Pentagon Build AI for Drones, by Kate Conger and Dell Cameron at Gizmodo 10.0.8 Slides and Video Ethical Data Science Slides "],["references.html", "Chapter 11 References", " Chapter 11 References "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) FirstName LastName Lecturer(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved Delivered the course in some way - video or audio Content Author(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved If any other authors besides lead instructor Content Contributor(s) (include section name/link in parentheses) - make new line if more than one section involved Wrote less than a chapter Content Editor(s)/Reviewer(s) Checked your content Content Director(s) Helped guide the content direction Content Consultants (include chapter name/link in parentheses or word “General”) - make new line if more than one chapter involved Gave high level advice on content Acknowledgments Gave small assistance to content but not to the level of consulting Production Content Publisher(s) Helped with publishing platform Content Publishing Reviewer(s) Reviewed overall content and aesthetics on publishing platform Technical Course Publishing Engineer(s) Helped with the code for the technical aspects related to the specific course generation Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) John Muschelli, Candace Savonen, Carrie Wright Art and Design Illustrator(s) Created graphics for the course Figure Artist(s) Created figures/plots for course Videographer(s) Filmed videos Videography Editor(s) Edited film Audiographer(s) Recorded audio Audiography Editor(s) Edited audio recordings Funding Funder(s) Institution/individual who funded course including grant number Funding Staff Staff members who help with funding   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.2 (2021-11-01) ## os macOS Big Sur 10.16 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2022-06-20 ## pandoc 2.14.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.0) ## bslib 0.3.1 2021-10-06 [1] CRAN (R 4.1.0) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.1.0) ## callr 3.7.0 2021-04-20 [1] CRAN (R 4.1.0) ## cli 3.1.0 2021-10-27 [1] CRAN (R 4.1.0) ## crayon 1.5.0 2022-02-14 [1] CRAN (R 4.1.2) ## desc 1.4.0 2021-09-28 [1] CRAN (R 4.1.0) ## devtools 2.4.3 2021-11-30 [1] CRAN (R 4.1.0) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.1.0) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) ## evaluate 0.14 2019-05-28 [1] CRAN (R 4.1.0) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.1.0) ## glue 1.6.0 2021-12-17 [1] CRAN (R 4.1.0) ## htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) ## jsonlite 1.7.2 2020-12-09 [1] CRAN (R 4.1.0) ## knitr 1.37 2021-12-16 [1] CRAN (R 4.1.0) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.1.0) ## magrittr 2.0.1 2020-11-17 [1] CRAN (R 4.1.0) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.1.0) ## pkgbuild 1.3.1 2021-12-20 [1] CRAN (R 4.1.0) ## pkgload 1.2.4 2021-11-30 [1] CRAN (R 4.1.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.1.0) ## processx 3.5.2 2021-04-30 [1] CRAN (R 4.1.0) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.1.0) ## purrr 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.0) ## remotes 2.4.2 2021-11-30 [1] CRAN (R 4.1.0) ## rlang 1.0.2 2022-03-04 [1] CRAN (R 4.1.2) ## rmarkdown 2.11 2021-09-14 [1] CRAN (R 4.1.0) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.1.0) ## sass 0.4.1 2022-03-23 [1] CRAN (R 4.1.2) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.1.0) ## stringi 1.7.6 2021-11-29 [1] CRAN (R 4.1.0) ## stringr 1.4.0 2019-02-10 [1] CRAN (R 4.1.0) ## testthat 3.1.1 2021-12-03 [1] CRAN (R 4.1.0) ## usethis 2.1.5 2021-12-09 [1] CRAN (R 4.1.0) ## withr 2.4.3 2021-11-30 [1] CRAN (R 4.1.0) ## xfun 0.29 2021-12-14 [1] CRAN (R 4.1.0) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 4.1.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
